{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Scholarag","text":"<p>This package is an SDK (software development kit) that allows for easy interaction with ML servers.</p>"},{"location":"concepts/","title":"Basic concepts","text":"<p>In this documentation, we will explore important concepts related to various endpoints in our application.</p>"},{"location":"concepts/#client-side-batching","title":"Client-side batching","text":""},{"location":"concepts/#error-codes","title":"Error Codes","text":"<p>The QA endpoint uses error codes to communicate specific issues to users. Codes 1,2,3 and 5,6 are associated with HTTP status code 500, while code 4 with HTTP status code 413.</p>"},{"location":"concepts/#no-database-entries-found-code-1","title":"No database entries found (code 1)","text":"<p>No relevant context was found in the database when using the QA or retrieval endpoints.</p> <pre><code>{\n    \"detail\": {\n        \"code\": 1,\n        \"detail\": \"No document found. Modify the filters or the query and try again.\"\n    }\n}\n</code></pre>"},{"location":"concepts/#no-answer-found-during-retrieval-code-2","title":"No answer found during retrieval (code 2)","text":"<p>Relevant contexts were found in the database when using the QA or retrieval endpoints, but the LLM could not give an answer based on these paragraphs.</p> <pre><code>{\n    \"detail\": {\n        \"code\": 2,\n        \"detail\": \"The LLM did not provide any source to answer the question.\",\n        \"raw_answer\": \"This is a raw answer.\"\n    }\n}\n</code></pre>"},{"location":"concepts/#endpoint-is-inactive-code-3","title":"Endpoint is inactive (code 3)","text":"<p>On of the services used by the application is inactive, or one of the index names is not set.</p>"},{"location":"concepts/#input-exceeded-max-tokens-code-4","title":"Input exceeded max tokens (code 4)","text":"<p>The input text's (The question + all the contexts sent) length is higher than acceptable (the context window of the LLM might be the bottleneck).</p> <pre><code>{\n    \"detail\": {\n        \"code\": 4,\n        \"detail\": \"OpenAI error.\"\n    }\n}\n</code></pre>"},{"location":"concepts/#maximum-number-of-cohere-requests-reached-code-5","title":"Maximum number of Cohere requests reached (code 5)","text":"<p>The maximum number of requests have been reached for the Cohere reranking model.</p> <pre><code>{\n    \"detail\": {\n        \"code\": 5,\n        \"detail\": \"Max number of requests reached for Cohere reranker. Wait a little bit and try again, or disable the reranker.\"\n    }\n}\n</code></pre>"},{"location":"concepts/#answer-is-incomplete-code-6","title":"Answer is incomplete (code 6)","text":"<p>Maximum number of tokens was reached when generating an answer.</p> <pre><code>{\n    \"detail\": {\n        \"code\": 6,\n        \"detail\": \"The LLM did not have enough completion tokens available to finish its answer. Please decrease the retriever_k value of 1 or 2.\",\n        \"raw_answer\": \"Some raw answer.\"\n    }\n}\n</code></pre>"},{"location":"concepts/#pipeline","title":"Pipeline","text":""},{"location":"deployment/","title":"Application Deployment","text":"<p>The <code>scholarag</code> package contains an application that one can deploy. To benefit from all the functionalities, some infrastructure needs to be deployed too:</p> <ul> <li>One needs first to deploy the application <code>scholarag</code> itself.</li> <li>A database containing the text content from scientific articles. The package currently supports <code>OpenSearch</code> and    <code>ElasticSearch</code> databases.</li> <li>(Optional) If the database needs to be populated, a tool parsing scientific articles is needed.    This is the case of <code>scholaretl</code> that is fully compatible with <code>scholarag</code>.    If <code>scholaretl</code> is used and some scientific papers are saved under <code>pdf</code> format,    one also needs to deploy a <code>grobid</code> server.</li> <li>(Optional) A database to be able to do caching. <code>Redis</code> is the only solution supported by <code>scholarag</code>.</li> </ul> <p>To deploy everything, one needs first to create fill the two following environment variables in the <code>compose.yaml</code> file. <pre><code>SCHOLARAG__DB__INDEX_PARAGRAPHS=TO_BE_SPECIFIED\nSCHOLARAG__GENERATIVE__OPENAI__TOKEN=TO_BE_SPECIFIED\n</code></pre></p> <p>Once this is done, one can simply use <code>docker compose up</code> command from the root folder of the package: <pre><code>docker compose up\n</code></pre> Five containers are then spawn: - <code>sholarag-app</code> containing the application that can be reached from <code>localhost:8080</code>. - The opensearch database is deployed under <code>opensearchproject/opensearch:2.5.0</code> called <code>scholarag-opensearch-1</code> and reachable on <code>localhost:9200</code>.  - <code>grobid/grobid:0.8.0</code> called <code>scholarag-grobid-1</code> being the grobid server reachable on <code>localhost:8070</code>. - The redis instance called <code>scholarag-redis-1</code> reachable on <code>localhost:6379</code>. - The ETL application <code>ETL IMAGE</code> called <code>scholarag-etl-1</code> reachable on port <code>9090</code>.</p> <p>To destroy everything, one can simply use the following command: <pre><code>docker compose down\n</code></pre> If one keeps the volumes setup inside the <code>compose.yaml</code> file, the data inside the <code>opensearch</code> database  is going to persist between different sessions. </p>"},{"location":"deployment/#database-population","title":"Database population","text":"<p>To populate the database with data and to use all the functionalities of the <code>scholarag</code> application, two indices need to be created:  - One containing the text content to use for the question answering - If text are coming from scientific papers, an index containing the impact factors of the different scientific journals.</p> <p>Both indexes can be created and populated through two scripts (also deployed as endpoints) available in <code>scholarag</code> package. For the first index, the script is <code>parse_and_upload.py</code> script (<code>pmc_parse_and_upload</code> can also be used if one wants to upload PMC papers to the database). For the second, one can launch <code>create_impact_factor_index.py</code>. Please refers to the <code>scripts</code> documentation for  further information. Both can be launched locally or after spawning a new docker container with the package installed with the  following command line: <pre><code>docker run -it --network=host scholarag-app /bin/bash\n</code></pre></p> <p>The flag <code>--network=host</code> is not mandatory but it is allowing a user to easily connect to the database deployed by referring to it as <code>http://localhost:9200</code>.</p> <p>As explained in the documentation, the script needs a <code>parser_url</code> as input.  We recommend to use <code>scholaretl</code>, a package fully compatible with <code>scholarag</code>, to populate the first index.  The purpose of the package is indeed to parse scientific articles with different formats and schemas (XML and PDF). Launching <code>docker compose</code> is spawning this <code>scholaretl</code> application that is then directly usable.</p> <p>The population of the database can then be launched using the following command (inside or outside a docker container): <pre><code>pmc-parse-and-upload http://localhost:9200 http://localhost:9090/parse/jats_xml \ncreate-impact-factors-index file_name impact_factors http://localhost:9200\n</code></pre> Note that the user needs to first create the index. By default, the index is called <code>pmc_paragraphs</code> but it can be changed by adding the flag <code>--index</code>.</p> <p>For the impact factor, one needs first to copy the file containing the information inside the docker  if the script is launched inside the docker container. To copy it, one can launch the command: <pre><code>docker cp file {DOCKER_CONTAINER}:/mnt\n</code></pre></p>"},{"location":"endpoints/","title":"Endpoints","text":""},{"location":"endpoints/#question-answering","title":"Question Answering","text":""},{"location":"endpoints/#qa-service-overview","title":"QA Service Overview","text":"<p>The QA service performs the following steps to generate answers:</p> <ol> <li> <p>Retrieval: The service retrieves relevant documents and contexts from the database using Elasticsearch (ES) or    OpenSearch (OS) with BM25. This step provides the initial set of information for generating answers.</p> </li> <li> <p>Reranker (Optional):The reranker takes a large number of retrieved paragraphs and employs a machine learning    model to re-sort the paragraphs based on semantic similarity to the user's question embedding. It returns the '    reranker_k' most similar paragraphs, refining the search results.</p> </li> <li>Answer Generation: The QA service generates answers using the retrieved documents and the user's question. It    uses a LLM generative model to compose responses based on the retrieved contexts.</li> </ol> <pre><code>sequenceDiagram\n  autonumber\n  SDK-&gt;&gt;DocumentStore: Retrieve semantically similar contexts\n  SDK-&gt;&gt;Reranker: Rerank contexts\n  SDK-&gt;&gt;DocumentStore: Retrieve document metadata\n  SDK-&gt;&gt;OpenAI: Ask question from LLM</code></pre>"},{"location":"endpoints/#calling-the-endpoint","title":"Calling the endpoint","text":"cURLPython <pre><code>curl --location 'https://ml.bbs.master.kcp.bbp.epfl.ch/qa/generative' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"query\": \"What is the number of cells in the human brain?\",\n    \"retriever_k\": 5,\n    \"reranker_k\": 4,\n    \"use_reranker\": true\n}'\n</code></pre> <pre><code>import requests\nimport json\n\nurl = \"https://ml.bbs.master.kcp.bbp.epfl.ch/qa/generative\"\n\npayload = json.dumps({\n    \"query\": \"What is the number of cells in the human brain?\",\n    \"retriever_k\": 5,\n    \"reranker_k\": 4,\n    \"use_reranker\": True\n})\nheaders = {\n  'Content-Type': 'application/json'\n}\n\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\n\nprint(response.text)\n</code></pre> Parameter Description <code>query</code> The question we want to ask. <code>retriever_k</code> The number of contexts to retrieve. <code>reranker_k</code> If use_reranker is true, we only retrieve the reranker_k best contexts. <code>use_reranker</code> If set to true, we use a ML model to re-sort the retrieved contexts according to semantic similarity to the query."},{"location":"endpoints/#response-format","title":"Response Format","text":"<p>The response format of the QA endpoint includes a final answer along with metadata about relevant sources. This structured format ensures that users receive comprehensive answers.</p> <pre><code>{\n  \"answer\": \"The number of cells in the human brain is approximately 86 billion.\",\n  \"raw_answer\": \"The number of cells in the human brain is approximately 86 billion.\\n&lt;bbs_sources&gt;: 0, 1\",\n  \"metadata\": [\n    {\n      \"article_title\": \"Development and Evolution of the Human Neocortex\",\n      \"article_authors\": [\n        \"Jan H. Lui\",\n        \"David V. Hansen\",\n        \"Arnold R. Kriegstein\"\n      ],\n      \"article_id\": \"e80c88ab244eb08290e6fded98d5b8e2\",\n      \"article_doi\": \"10.1016/j.cell.2011.06.030\",\n      \"pubmed_id\": \"21729779\",\n      \"date\": \"2017-03-01\",\n      \"article_type\": \"research-article\",\n      \"journal_issn\": \"0092-8674\",\n      \"journal_name\": \"Cell (Cambridge)\",\n      \"cited_by\": 1097,\n      \"impact_factor\": 1.0,\n      \"abstract\": \"The size and surface area of the mammalian brain are thought to be critical determinants of intellectual ability. Recent studies show that development of the gyrated human neocortex involves a lineage of neural stem and transit-amplifying cells that forms the outer subventricular zone (OSVZ), a proliferative region outside the ventricular epithelium. We discuss how proliferation of cells within the OSVZ expands the neocortex by increasing neuron number and modifying the trajectory of migrating neurons. Relating these features to other mammalian species and known molecular regulators of the mouse neocortex suggests how this developmental process could have emerged in evolution.\",\n      \"paragraph\": \"Studies of the human brain and comparisons of developmental proliferative zones between species may ultimately help to explain what makes the human brain unique. It is commonly thought that the exceptional cognitive abilities of humans are related to the large size of the neocortex. Recent evidence has shown that primates have greater neuronal density (neuron number/brain mass) compared to rodents of equal brain mass, a feature that is likely related to the topological differences in foldedness that could have been influenced by OSVZ proliferation. However, though the human brain is large by weight (1.5 kg) and neuron number (86 billion) (Azevedo et al., 2009; reviewed by Herculano-Houzel, 2009), this ratio does not deviate from what would be expected from a primate brain of similar mass, implying that, in terms of brain size and density, the human brain conforms to a scaled-up primate brain. Furthermore, developmental similarities between the human and ferret show that increased OSVZ proliferation and oRG cells are not primate-specific features (Figure 7A). The percentage of progenitor cells in the SVZ/OSVZ of rodents, carnivores, ungulates, and primates shows a remarkable positive correlation with the degree of neocortical gyrification (Reillo et al., 2010). Thus, development of OSVZ proliferation appears to be an important general feature for increasing neuronal number and neocortical surface area throughout Eutheria.\",\n      \"paragraph_id\": \"9bb53ebb0f82d2a848c7ef5c1a434293\",\n      \"context_id\": 0,\n      \"reranking_score\": null,\n      \"section\": \"The Human Neocortex: A Scaled-up Primate Brain\"\n    }\n  ]\n}\n</code></pre>"},{"location":"endpoints/#return-format-properties","title":"Return format properties","text":"Parameter Description <code>answer</code> The actual answer to the question. <code>raw_answer</code> This is the raw text returned by the Large Language Model. If the pipeline successfully found an answer this will contain  it, as well as a list of sources. <code>metadata</code> A list where the elements are objects containing key-value pairs. Each objects has metadata about one of the source paragraphs that was retreived."},{"location":"endpoints/#metadata-properties","title":"Metadata properties","text":"Metadata parameter Description <code>article_title</code> Title of this article. <code>article_authors</code> A list of authors. <code>article_id</code> Unique identifier of this article in our database. <code>article_doi</code> Digital Object Identifier of this article. <code>pubmed_id</code> A unique identifier used in the PubMed database. <code>date</code> Publication date of this article. <code>article_type</code> The type of scholarly article. Some valid types are: research, publication, review, thesis <code>journal_issn</code> The International Standard Serial Number is a unique identifier for journals. <code>journal_name</code> The name of the journal where this article was published. <code>cited_by</code> The number of citations for this article. <code>impact_factor</code> The impact factor of the journal where this article was published. <code>abstract</code> The text of the abstract for this article. <code>paragraph</code> The actual text of this paragraph. <code>paragraph_id</code> The unique identifier of this paragraph in our database. <code>context_id</code> The unique index of this paragraph that is referenced in the returned answers. <code>reranking_score</code> Score returned by the reranker used to sort contexts by relevance. <code>section</code> Name of the section inside the publication where this paragraphs is from."},{"location":"endpoints/#separators","title":"Separators","text":"<p>The QA endpoint uses the following separators within its prompt template:</p> <ul> <li> <p>Sources Separator (<code>SOURCES_SEPARATOR</code>): This placeholder is used to indicate where references to sources should   be inserted in the final answer. It ensures that users can access the sources related to the provided answer.</p> </li> <li> <p>Error Separator (<code>ERROR_SEPARATOR</code>): The error separator is used to signify that an answer is not available. When   an answer is not found, the response starts with the error separator, providing transparency about the absence of a   valid response.</p> </li> </ul>"},{"location":"endpoints/#retrieval","title":"Retrieval","text":"<p>One can also retrieve data from the database without the call to a Large Language Model. </p>"},{"location":"endpoints/#article-count","title":"Article count","text":"<p>The article count endpoint is a convenient endpoint that returns the number of articles in our DB that match certain filters that the user can specify. The filters are described in the Filtering section.</p>"},{"location":"endpoints/#calling-the-endpoint_1","title":"Calling the endpoint","text":"cURLPython <pre><code>curl --location 'https://ml.bbs.master.kcp.bbp.epfl.ch/retrieval/article_count?topics=pyramidal%20cells'\n</code></pre> <pre><code>import requests\nimport json\n\nurl = \"https://ml.bbs.master.kcp.bbp.epfl.ch/retrieval/article_count?topics=pyramidal%20cells\"\n\nresponse = requests.request(\"GET\", url)\n</code></pre>"},{"location":"endpoints/#response-format_1","title":"Response Format","text":"<p>The response format of the <code>article_count</code> endpoint is a very simple JSON. <pre><code>{\"article_count\": 29203}\n</code></pre></p>"},{"location":"endpoints/#article-listing","title":"Article listing","text":"<p>The <code>article_listing</code> endpoint is complementary to the <code>article_count</code> one. The articles counted in <code>article_count</code> can be displayed using this endpoint, granted that the same filters are applied.</p>"},{"location":"endpoints/#calling-the-endpoint_2","title":"Calling the endpoint","text":"cURLPython <pre><code>curl --location 'https://ml.bbs.master.kcp.bbp.epfl.ch/retrieval/article_listing?topics=pyramidal%20cells'\n</code></pre> <pre><code>import requests\nimport json\n\nurl = \"https://ml.bbs.master.kcp.bbp.epfl.ch/retrieval/article_listing?topics=pyramidal%20cells\"\n\nresponse = requests.request(\"GET\", url)\n</code></pre> Parameter Description <code>filters</code> Typical retrieval filters. <code>number_results</code> Maximum number of unique articles to retrieve from the DB per request. <code>size</code> Number of unique articles per page. <code>page</code> Page number to retrieve <p>Note</p> <p>The pagination is not a real pagination. For each request, our DB returns <code>number_results</code> unique articles, that we split in <code>ceil(number_results / size)</code> pages. Specifying a page number or a size such that <code>page * size &gt; number_results</code> will result in an empty or partially empty output.</p> <p>Note</p> <p>Considering the way we paginate results, each new page retrieved fetches <code>number_results</code> articles from our DB and fetches the relevant metadata, leading to a potential heavy load on the DB. For that reason, and at least while we are hosting our DB on kubernetes, <code>number_results</code> cannot be too high to avoid crashes.</p>"},{"location":"endpoints/#response-format_2","title":"Response Format","text":"<p>The response format of the <code>article_count</code> endpoint is shown as a JSON containing the relevant metadata about the selected articles.</p> <p><pre><code>{\n  \"items\": [\n    {\n      \"article_title\": \"Suppression of Neuronal Firing Following Antidromic High-Frequency Stimulations on the Neuronal Axons in Rat Hippocampal CA1 Region\",\n      \"article_authors\": [\n        \"Yue Yuan\",\n        \"Zhouyan Feng\",\n        \"Gangsheng Yang\",\n        \"Xiangyu Ye\",\n        \"Zhaoxiang Wang\"\n      ],\n      \"article_id\": \"6d0b455cbdbf594fcf1c22228f9671df\",\n      \"article_doi\": \"10.3389/fnins.2022.881426\",\n      \"pubmed_id\": null,\n      \"date\": \"2022-06-10\",\n      \"article_type\": \"research-article\",\n      \"journal_issn\": \"1662-4548\",\n      \"journal_name\": \"Frontiers in neuroscience (Print)\",\n      \"cited_by\": 0,\n      \"impact_factor\": null,\n      \"abstract\": \"High-frequency stimulation (HFS) of electrical pulses has been used to treat certain neurological diseases in brain with commonly utilized effects within stimulation periods. Post-stimulation effects after the end of HFS may also have functions but are lack of attention. To investigate the post-stimulation effects of HFS, we performed experiments in the rat hippocampal CA1 region in vivo. Sequences of 1-min antidromic-HFS (A-HFS) were applied at the alveus fibers. To evaluate the excitability of the neurons, separated orthodromic-tests (O-test) of paired pulses were applied at the Schaffer collaterals in the period of baseline, during late period of A-HFS, and following A-HFS. The evoked potentials of A-HFS pulses and O-test pulses were recorded at the stratum pyramidale and the stratum radiatum of CA1 region by an electrode array. The results showed that the antidromic population spikes (APS) evoked by the A-HFS pulses persisted through the entire 1-min period of 100 Hz A-HFS, though the APS amplitudes decreased significantly from the initial value of 9.9 \u00b1 3.3 mV to the end value of 1.6 \u00b1 0.60 mV. However, following the cessation of A-HFS, a silent period without neuronal firing appeared before the firing gradually recovered to the baseline level. The mean lengths of both silent period and recovery period of pyramidal cells (21.9 \u00b1 22.9 and 172.8 \u00b1 91.6 s) were significantly longer than those of interneurons (11.2 \u00b1 8.9 and 45.6 \u00b1 35.9 s). Furthermore, the orthodromic population spikes (OPS) and the field excitatory postsynaptic potentials (fEPSP) evoked by O-tests at \u223c15 s following A-HFS decreased significantly, indicating the excitability of pyramidal cells decreased. In addition, when the pulse frequency of A-HFS was increased to 200, 400, and 800 Hz, the suppression of neuronal activity following A-HFS decreased rather than increased. These results indicated that the neurons with axons directly under HFS can generate a post-stimulation suppression of their excitability that may be due to an antidromic invasion of axonal A-HFS to somata and dendrites. The finding provides new clues to utilize post-stimulation effects generated in the intervals to design intermittent stimulations, such as closed-loop or adaptive stimulations.\"\n    }\n  ],\n  \"total\": 100,\n  \"page\": 1,\n  \"size\": 1,\n  \"pages\": 100\n}\n</code></pre> The meaning of each fields can be found here</p>"},{"location":"endpoints/#journal-suggestion","title":"Journal suggestion","text":"<p>TODO</p>"},{"location":"endpoints/#author-suggestion","title":"Author suggestion","text":"<p>TODO</p>"},{"location":"endpoints/#filtering","title":"Filtering","text":"<p>Many endpoints related to document retrieval from the database share the same set of query parameters that can help further filtering the result of the retrieval. These query parameters are optional and should usually be proposed to the end user to filter his search. This section describes them in details.</p>"},{"location":"endpoints/#topics","title":"topics","text":"<p>Indicates which topic(s) the article should be about. You can specify multiple topics using the format <code>topics=xxx&amp;topics=yyy</code>. If multiple topics are specified, they are AND matched. If a topic consists of multiple words, each word is AND matched. For instance, specifying <code>topics=[neuron morphology, electrophysiololgy]</code> results in the query :</p> <pre><code>{\"query\": \"((neuron AND morphology) AND electrophysiololgy)\"}\n</code></pre> <p>and only paragraphs containing the words \"neuron\" AND \"morpholgy\" AND \"electrophysiology\" will be returned.</p> <p>Note</p> <p>\"neuron morphology\" doesn't necessarily have to be present as a whole, each word can be scattered around. For endpoints dealing with articles instead of paragraphs, an article will be returned if at least one of its paragraphs matches the filter.</p>"},{"location":"endpoints/#regions","title":"regions","text":"<p>Indicates which region(s) (of the brain) the article should be about. You can specify multiple regions using the format <code>regions=xxx&amp;regions=yyy</code>. If multiple regions are specified, they are OR matched. If a region consists of multiple words, each word is AND matched. For instance, specifying <code>regions=[frontal lobe, isocortex]</code> results in the query:</p> <pre><code>{\"query\": \"((frontal AND lobe) OR isocortex)\"}\n</code></pre> <p>and only paragraphs containing the words \"frontal + lobe\" OR \"isocortex\" will be returned.</p> <p>Note</p> <p>\"frontal lobe\" doesn't necessarily have to be present as a whole, each word can be scattered around. For endpoints dealing with articles instead of paragraphs, an article will be returned if at least one of its paragraphs matches the filter.</p> <p>Note</p> <p>If topics and regions are used together, they are AND matched. for instance, specifying <code>topics=[neuron morphology, electrophysiololgy]</code> and <code>regions=[frontal lobe, isocortex]</code> results in the query <pre><code>{\"query\": \"((neuron AND morphology) AND electrophysiololgy) AND ((frontal AND lobe) OR isocortex)\"}\n</code></pre></p>"},{"location":"endpoints/#article_types","title":"article_types","text":"<p>Specifies the type(s) of article that one is looking for. Article types and their occurence are listed by the <code>/suggestions/article_types</code> endpoints, which can be used to suggests results for this filter. Examples of article types could be <code>research-article</code> or <code>review-article</code> for instance. Multiple article types can be specified using the syntax <code>article_types=xxx&amp;article_types=yyy</code>.</p> <p>Note</p> <p>If multiple article types are specified, they are OR matched (i.e. the article should be of type <code>xxx</code> OR <code>yyy</code> OR...).</p>"},{"location":"endpoints/#authors","title":"authors","text":"<p>Specifies which author(s) should have written the article. The endpoint <code>/suggestions/author</code> is made available to suggest authors based on partial name matching to help the user fill this filter. Multiple authors can be specified using the syntax <code>authors=xxx&amp;authors=yyy</code>.</p> <p>Note</p> <p>If multiple authors are specified, they are OR matched (i.e. the article should have been written by <code>xxx</code> OR <code>yyy</code> OR...).</p>"},{"location":"endpoints/#journals","title":"journals","text":"<p>Specifies in which journal(s) the article should be published. This query parameter expects the ISSN of the journal in the format <code>XXXX-XXXX</code>, not its name. The endpoint <code>/suggestions/journal</code> is made available to suggest journals based on name partial matching to help the user fill this filter. Multiple journals can be specified using the syntax <code>journals=xxx&amp;journals=yyy</code>.</p> <p>Note</p> <p>If multiple journals are specified, they are OR matched (i.e. the article should have been published in <code>XXXX-XXXX</code> OR <code>YYYY-YYYY</code> OR...).</p>"},{"location":"endpoints/#date_from","title":"date_from","text":"<p>Lower bound on the publication date of the article. Format: <code>YYYY-MM-DD</code></p>"},{"location":"endpoints/#date_to","title":"date_to","text":"<p>Upper bound on the publication date of the article. Format: <code>YYYY-MM-DD</code></p>"},{"location":"examples/","title":"Examples","text":""},{"location":"reference/","title":"Reference","text":"<p>This is just a demo of how it could be used. See https://mkdocstrings.github.io/ for all the details.</p> <p>Possible objects to pull the docs from. <pre><code>::: scholarag.module\n::: scholarag.subpackage.module\n::: scholarag.module.class\n</code></pre></p>"},{"location":"scripts/","title":"Scripts","text":"<p>Scholarag comes shipped with several handy scripts that help with filling up a database with documents used in the main API. They allow the user to parse and upload articles from several sources, while pre-defining the various fields in the DB that should contain article's metadata. Combined with the parsers offered in <code>bbs-etl</code>, they offer an easy solution to have a production grade database based on several data sources. There exist five main scripts:</p> <pre><code>create_impact_factor_index.py\nmanage_index.py\nparse_and_upload.py\npmc_parse_and_upload.py\npu_producer.py + pu_consumer.py\n</code></pre>"},{"location":"scripts/#create_impact_factor_indexpy","title":"create_impact_factor_index.py","text":"<p>This script is creating the index containing impact factors of scientific journals.  Data to populate the index should be saved under an excel file with the same schema as  the one that can be found under <code>tests/data/citescore_sample.xlsx</code>. This file can be asked to https://www.elsevier.com/products/scopus/metrics/citescore. </p> <p>Once the file is obtained, one can simply run the script by specifying the path to the file, the name of the index one wants to have and the database where to save it.</p> <p>After creating this index, one can use it in the application by specifying the environment variable <code>SCHOLARAG__DB__INDEX_JOURNALS</code> with the new index name. Impact factors of the journals are going to be given to the user when returning articles metadata.</p> <pre><code>positional arguments:\n  from_file             Path to file containing impact factor information.\n  index                 Name of the index to create.\n  db_url                URL of the database instance. Format: HOST:PORT\n\noptions:\n  -h, --help            show this help message and exit\n  --db-type {elasticsearch,opensearch}\n                        Type of database. (default: opensearch)\n  --user USER           User of the database instance. The password is read from the environment under SCHOLARAG__DB__PASSWORD. (default: None)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre>"},{"location":"scripts/#manage_indexpy","title":"manage_index.py","text":"<p>This script is a very simple database setup script. It allows for three concrete actions on a specific index: <code>create</code>, <code>delete</code>, <code>reset</code>.</p> <ul> <li> <p>create: Create the specified index with a mapping compatible with what the main API expects, ready to receive documents from the parsers on <code>bbs-etl</code>. Several options can be provided to customise the index, but always within the usecase of the main API.</p> </li> <li> <p>delete: Simply delete the specified index if it exists.</p> </li> <li> <p>reset: Apply first delete, then create. It is a handy way to erase everything in your index for experimentation.</p> </li> </ul> <p>The remaining arguments of the script are typical arguments to point to the db, connect to it, and have a small customization capability. Here is an exhaustive list:</p> <p><pre><code>positional arguments:\n  {create,delete,reset}\n                        Specify the action to take. Reset = delete -&gt; create.\n  index                 Name of the index to deal with.\n  db_url                URL of the database instance. Format: HOST:PORT\n\noptions:\n  -h, --help            show this help message and exit\n  --db-type {elasticsearch,opensearch}\n                        Type of database. (default: elasticsearch)\n  --user USER           User of the database instance. The password is read from the environment under SCHOLARAG__DB__PASSWORD. (default: None)\n  --embed-name EMBED_NAME\n                        Name of the embedding field. (default: embed_multi-qa-mpnet-base-dot-v1:1_0_0)\n  --embed-dims EMBED_DIMS\n                        Dimension of the embedding vectors. (default: 768)\n  --n-shards N_SHARDS   Number of shards for the index. (default: 2)\n  --n-replicas N_REPLICAS\n                        Number of replicas for the index. (default: 1)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre> The password of the database, if it exists, should be set in the environment under the variable name <code>SCHOLARAG__DB__PASSWORD</code>.</p>"},{"location":"scripts/#parse_and_uploadpy","title":"parse_and_upload.py","text":"<p>This script parses local files (on your computer) and uploads them to the database you point to. It leverages the parsers in <code>bbs-etl</code> to extract the information from your xml/pdf files as well as the mapping provided by <code>manage_index.py</code>. Make sure to create an index using the latter before running this script. Here is an exhaustive list of the script's arguments:</p> <p><pre><code>positional arguments:\n  path                  File or directory where are located the articles to parse.\n  parser_url            URL of the parser (needs to contain this address and the parser).\n  db_url                URL of the database instance (together with the port).\n\noptions:\n  -h, --help            show this help message and exit\n  --articles-per-bulk ARTICLES_PER_BULK\n                        Maximum number of articles to process before sending them to the db. (default: 1000)\n  --multipart-params MULTIPART_PARAMS\n                        Dictionary for the multipart parameters for parsers that require some. Must be passed as a json formated dict. (default: None)\n  --max-concurrent-requests MAX_CONCURRENT_REQUESTS\n                        Maximum number of articles sent to the etl server PER BATCH OF 'articles-per-bulk' articles. (default: 10)\n  --db-type {elasticsearch,opensearch}\n                        Type of database. (default: opensearch)\n  --user USER           User of the database instance. The password is read from the environment under SCHOLARAG__DB__PASSWORD. (default: None)\n  --index INDEX         Desired name of the index holding paragraphs. (default: paragraphs)\n  --files-failing-path FILES_FAILING_PATH\n                        Path where to dump the files that failed. Expects a file. (default: None)\n  -m MATCH_FILENAME, --match-filename MATCH_FILENAME\n                        Parse only files with a name matching the given regular expression. Ignored when 'input_path' is a path to a file. (default: None)\n  -r, --recursive       Parse files recursively. (default: False)\n  --use-ssl             Whether to verify ssl certificates or not. (default: False)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre> The parser specified under <code>parser_url</code> MUST contain the endpoint, which means that one must carefully select the correct parser for their data. For instance, xmls originating from PMC should use the <code>jats_xml</code> parser. Xmls originating from scopus must use the <code>xocs_xml</code> parser. Pdfs should use the <code>grobid_pdf</code> parser and so on. Please refer to the <code>bbs-etl</code> documentation for more information.</p>"},{"location":"scripts/#pmc_parse_and_uploadpy","title":"pmc_parse_and_upload.py","text":"<p>This script is very similar to <code>parse_and_upload.py</code>, except that it doesn't use local files but rather fetches the files located in the public AWS s3 bucket of PMC. This bucket contains an up to date dump of the entirety of PMC (representing at this time roughly 6M articles). It uses <code>boto3</code> and <code>aiobotocore</code> to fetch and download files from the s3 bucket, and doesn't require any specific permission or access. It is a great way to download rapidly a big volume of data for experimentation.  It leverages the parsers in <code>bbs-etl</code> to extract the information from the xmls as well as the mapping provided by <code>manage_index.py</code>. Make sure to create an index using the latter before running this script. Here is an exhaustive list of the script's arguments:</p> <p><pre><code>positional arguments:\n  db_url                URL of the database instance (together with the port).\n  parser_url            URL of the parser (needs to contain the host:port and the parser type).\n\noptions:\n  -h, --help            show this help message and exit\n  --start-date START_DATE\n                        Date of the oldest document to download. Format: dd-mm-yyy. (default: None)\n  --batch-size BATCH_SIZE\n                        Maximum number of articles to process before sending them to the db. (default: 500)\n  --multipart-params MULTIPART_PARAMS\n                        Dictionary for the multipart parameters for parsers that require some. Must be passed as a json formated dict. (default: None)\n  --max-concurrent-requests MAX_CONCURRENT_REQUESTS\n                        Maximum number of articles sent to the etl server PER BATCH OF 'articles-per-bulk' articles. (default: 10)\n  --db-type {elasticsearch,opensearch}\n                        Type of database. (default: opensearch)\n  --user USER           User of the database instance. The password is read from the environment under SCHOLARAG__DB__PASSWORD. (default: None)\n  --index INDEX         Desired name of the index holding paragraphs. (default: pmc_paragraphs)\n  --files-failing-path FILES_FAILING_PATH\n                        Path where to dump the files that failed. Expects a file. Not dumping if None. (default: None)\n  --use-ssl             Whether to verify ssl certificates or not. (default: False)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre> The correct parser endpoint to use for this script is <code>jats_xml</code> since every file originates from PMC. If <code>--start-date</code> remains <code>None</code>, it will be set to the previous day so that if you run this script automatically every day, you would always fetch the latest updates.</p>"},{"location":"scripts/#pu_producerpy-pu_consumerpy","title":"pu_producer.py + pu_consumer.py","text":"<p>These scripts should be used together. They leverage an AWS service called SQS quich provides a queue service. They are the most efficient way to efficiently populate a production database with a huge amount of files.</p>"},{"location":"scripts/#pu_producerpy","title":"pu_producer.py","text":"<p>When it comes to using a queue service, the producer has the role of putting messages in the queue for the consumer to consume them. Here, the producer fetches articles in a specific AWS s3 bucket, gets the relevant article path, and forwards this information into the queue. It also includes a bunch of metadata into the message, so that the consumer knows what to do. Here is an exhaustive list of the script's arguments:</p> <pre><code>positional arguments:\n  bucket_name           Url of the provider where to download the articles.\n  queue_url             Url of the queue where to upload the articles.\n\noptions:\n  -h, --help            show this help message and exit\n  --index INDEX         Desired name of the index holding paragraphs. (default: pmc_paragraphs)\n  --parser-name PARSER_NAME\n                        Endpoint of the parser to use. Specify only if the s3 bucket is not ours. Else it will be fetch from the path. (default: None)\n  --start-date START_DATE\n                        Date of the oldest document to download. Format: dd-mm-yyy. (default: None)\n  --prefixes PREFIXES [PREFIXES ...]\n                        Prefix in the s3 path to filter the search on. Can write multiple prefixes to make it a list. (default: None)\n  --sign-request        Sign the request with AWS credential. Should be activated for our bucket, not for public external buckets. (default: False)\n  --file-extension FILE_EXTENSION\n                        Extension of the file to enqueue. Leave None to parse any file type. (default: None)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre> <p>This script can work with any s3 bucket in theory. However if you point to a private s3 bucket, you need to sign your request with your AWS credentials using <code>--sign-request</code>. The <code>--prefixes</code> argument can be a list of partial paths inside of the s3 bucket, which will restrict the bucket crawling to the specified values. Finally <code>--file-extension</code> can be used to partially match the name of the files of interest, for instance by specifying only a certain type of files. The <code>--parser-name</code> is of very specific interest. If you point this script towards an external s3 bucket (for instance the PMC one), it is likely that every file will need to be parsed with the same parser (e.g. <code>jats_xml</code> for the PMC bucket). However, to remain compatible with any data provider, even those not having an s3 bucket, we offer the possibility to infer the type of parser from the name of the folder containing the data. For instance assume that you have the five following parsers: <code>jats_xml</code>, <code>xocs_xml</code>, <code>grobid_pdf</code>, <code>pubmed_xml</code> and <code>core_json</code>. Assume that you have a private s3 bucket with the following layout: <pre><code>my-s3-bucket\n\u251c\u2500\u2500 core_json\n\u2502   \u251c\u2500\u2500 file1.json\n\u2502   \u251c\u2500\u2500 file2.json\n\u2502\n\u251c\u2500\u2500 jats_xml\n\u2502   \u251c\u2500\u2500 file1.xml\n\u2502   \u251c\u2500\u2500 file2.xml\n\u2502\n\u251c\u2500\u2500 grobid_pdf\n\u2502   \u251c\u2500\u2500 file1.pdf\n\u2502   \u251c\u2500\u2500 file2.pdf\n\u2502\n\u251c\u2500\u2500 xocs_xml\n\u2502   \u251c\u2500\u2500 file1.xml\n\u2502   \u251c\u2500\u2500 file2.xml\n\u2502\n\u251c\u2500\u2500 pubmed_xml\n\u2502   \u251c\u2500\u2500 file1.xml\n\u2502   \u251c\u2500\u2500 file2.xml\n</code></pre> If you run the script without providing <code>--parser-name</code> and pointing to this s3 bucket, each file will be sent to the queue alongside with the name of its parent folder as the parser to use for parsing. Therefore, one can download locally any file (compatible with the parsers) from any provider, and simply dump them in the s3 bucket under the correct folder for parsing.</p>"},{"location":"scripts/#pu_consumerpy","title":"pu_consumer.py","text":"<p>The consumer script is complementary to the producer one. As soon as one runs the producer scripts, messages (articles) will be put in the sqs queue. The role of the consumer is to retrieve these messages, parse them and upload them to your database (that has a compatible index created using the <code>manage_index.py</code> script). Here is an exhaustive list of the script's arguments:</p> <p><pre><code>positional arguments:\n  db_url                URL of the database instance (together with the port).\n  parser_url            URL of the parser (needs to contain the host:port).\n  queue_url             Url of the queue where to upload the articles.\n\noptions:\n  -h, --help            show this help message and exit\n  --batch-size BATCH_SIZE, -b BATCH_SIZE\n                        Maximum number of articles to process before sending them to the db. (default: 500)\n  --max-concurrent-requests MAX_CONCURRENT_REQUESTS\n                        Maximum number of articles sent to the etl server PER BATCH OF 'articles-per-bulk' articles. (default: 10)\n  --db-type {elasticsearch,opensearch}\n                        Type of database. (default: opensearch)\n  --user USER           User of the database instance. The password is read from the environment under SCHOLARAG__DB__PASSWORD. (default: None)\n  --use-ssl             Whether to verify ssl certificates or not. (default: False)\n  -v, --verbose         Control verbosity (default: False)\n</code></pre> Here the <code>parser_url</code> MUST NOT contain the endpoint, since the endpoint is passed alongside with the message and is then retrieved individually for each message. The consumer script is a simple script that runs forever, doing what we call \"long polling\" of the queue. It will continuously listen to the queue, and process messages as soon as some are encountered in the queue. It implements multiprocessing capabilities to parallelize the processing of multiple messages at once. Typically useful to deploy it on a server and let it run continuously.</p>"},{"location":"setup/","title":"Application Setup","text":"<p>If one wants to run the application locally, it is needed to setup some environment variables first through the CLI or an <code>.env</code> file.</p>"},{"location":"setup/#mandatory-environment-variables","title":"Mandatory environment variables","text":"<p>Here is the list of environment variables that needs to be set up to be able to launch the application:</p> <ul> <li><code>SCHOLARAG__DB__DB_TYPE_</code>: the type of database used to store the articles. One can choose between <code>opensearch</code> or <code>elasticsearch</code>.</li> <li><code>SCHOLARAG__DB__INDEX_PARAGRAPHS</code>: the name of the index where the articles are stored.</li> <li><code>SCHOLARAG__DB__HOST</code>: host name of the OS/ES database.</li> <li><code>SCHOLARAG__DB__PORT</code>: port of the OS/ES database.</li> </ul>"},{"location":"setup/#optional-environment-variables","title":"Optional environment variables","text":"<p>Here is the list of environment variables that can be set up:</p>"},{"location":"setup/#related-to-the-oses-database","title":"Related to the OS/ES database","text":"<ul> <li><code>SCHOLARAG__DB__INDEX_JOURNALS</code>: the name of the name where the journals are stored.</li> <li><code>SCHOLARAG__DB__USER</code>: if authentication is needed, specify here the username of the OS/ES database.</li> <li><code>SCHOLARAG__DB__PASSWORD</code>: if authentication is needed, specify here the password of the given username for the OS/ES database.</li> </ul>"},{"location":"setup/#related-to-the-retrieval-of-the-documents","title":"Related to the retrieval of the documents","text":"<ul> <li><code>SCHOLARAG__RETRIEVAL__MAX_LENGTH</code>: maximum length of the documents to keep. By default, the value is 100000.</li> </ul>"},{"location":"setup/#related-to-the-language-model","title":"Related to the language model","text":"<p>Model <code>openai</code> is used for the LLM part, here are the environment variables to set up:</p> <ul> <li><code>SCHOLARAG__GENERATIVE__OPENAI__MODEL</code>: OpenAI model to use. By default, the model is <code>gpt-3.5-turbo</code>.</li> <li><code>SCHOLARAG__GENERATIVE__OPENAI__TOKEN</code>: OpenAI token.</li> <li><code>SCHOLARAG__GENERATIVE__OPENAI__TEMPERATURE</code>: Temperature of the model. By default, the temperature is 0.</li> <li><code>SCHOLARAG__GENERATIVE__OPENAI__MAX_TOKENS</code>: Maximum number of tokens for the language model to generate.</li> </ul> <p>If one wants to have a custom prompt template:</p> <ul> <li><code>SCHOLARAG__GENERATIVE__PROMPT_TEMPLATE</code>: Custom prompt template, expect {SOURCES_SEPARATOR} and {ERROR_SEPARATOR}, for more details have a look at the default one present in generative_question_answering.py.</li> </ul>"},{"location":"setup/#related-to-the-reranker","title":"Related to the reranker","text":"<p>To use the <code>cohere reranker</code>, this environment variable needs to be set up:</p> <ul> <li><code>SCHOLARAG__RERANKING__COHERE__TOKEN</code>: the cohere token.</li> </ul>"},{"location":"setup/#related-to-the-caching","title":"Related to the caching","text":"<p>If one wants to include some caching mechanisms:</p> <ul> <li><code>SCHOLARAG__REDIS__HOST</code>: the url of the caching database.</li> <li><code>SCHOLARAG__REDIS__PORT</code>: the port of the caching database.</li> <li><code>SCHOLARAG__REDIS__EXPIRY</code>: time in days for the expiration of the caching keys. By default, the value is 30.0.</li> </ul>"},{"location":"setup/#related-to-the-logging","title":"Related to the logging","text":"<ul> <li><code>SCHOLARAG__LOGGING__LEVEL</code>: the logging level of the application and the <code>scholarag</code> package logging. By default, the value is <code>info</code>.</li> <li><code>SCHOLARAG__LOGGING__EXTERNAL_PACKAGES</code>: the logging level of the external packages. By default, the value is <code>warning</code>.</li> </ul>"},{"location":"setup/#related-to-the-retrieval-of-metadata-of-the-documents","title":"Related to the retrieval of metadata of the documents","text":"<ul> <li><code>SCHOLARAG__METADATA__EXTERNAL_APIS</code>: boolean to decide if there are retrieval of the metadata or not. By default, the value is <code>True</code>.</li> <li><code>SCHOLARAG__METADATA__TIMEOUT</code>: timeout of the metadata retrieval requests. By default, the value is 30.</li> </ul>"},{"location":"setup/#misc-variables","title":"Misc variables","text":"<ul> <li><code>SCHOLARAG__MISC__APPLICATION_PREFIX</code>: Adds a prefix before every endpoint, which internally is removed by a middleware. Useful for instance for AWS application load balancer when using <code>path_patterns</code> conditions.</li> <li><code>SCHOLARAG__MISC__CORS_ORIGINS</code>: Specifies the cors origins to allow. Should be a string with comma separated values, i.e. <code>\"value_1, value_2, ...\"</code>.</li> </ul>"},{"location":"setup/#related-to-the-keycloak-authentication","title":"Related to the keycloak authentication","text":"<ul> <li><code>SCHOLARAG__KEYCLOAK__ISSUER</code>: Endpoint to use to check that the Keycloak token is valid.</li> <li><code>SCHOLARAG__KEYCLOAK__VALIDATE_TOKEN</code>: Boolean to decide if keycloak token should be validated or not.</li> </ul>"},{"location":"setup/#sentry-related","title":"Sentry related","text":"<p>If ones want to setup sentry, it is also possible via <code>SENTRY_DSN</code> and <code>SENTRY_ENVIRONMENT</code>.</p>"}]}